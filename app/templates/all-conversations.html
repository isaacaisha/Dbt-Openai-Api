{% extends 'header.html' %}

{% block title %} Welcomâ‚¬ to{% endblock %}

{% block head %}
    {{ super() }}
{% endblock %}

{% block content %}
    {% for item in data %}
        <textarea class="form-control" style="color:green; height: 109px;">{{ item }}</textarea><br>
    {% endfor %}

<hr class="mb-3" style="color:yellow;">

<div class="text-center">
    <a href="/" class="btn text-center btn-story mb-3" style="font-size:1rem;">Back Home -Â¡!Â¡-</a>
</div>

<hr style="color:green;">

{% endblock %}

{% block footer %} {% endblock %}



<!--
import openai
import uvicorn
import os
import psycopg2
import secrets
import time
import pytz
import json

from flask import send_file, request, jsonify
from sqlalchemy.orm import Session
from sqlalchemy import func

from fastapi import Response, status, HTTPException, Depends, APIRouter, FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import Form
from fastapi.responses import JSONResponse
from fastapi.responses import RedirectResponse

from .models import Memory
from . import models, schemas, oauth2
from .database import get_db
from . import models
from .database import engine
from .routers import conversation, user, auth
from dotenv import load_dotenv, find_dotenv
from app.schemas import TextAreaForm

from typing import List

from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory

from gtts import gTTS
from datetime import datetime

_ = load_dotenv(find_dotenv())

models.Base.metadata.create_all(bind=engine)

app = FastAPI(debug=True)

# Mount the 'static' folder for serving static files like CSS, JS, and images
app.mount("/static", StaticFiles(directory="./app/static"), name="static")

# Create an instance of the Jinja2Templates class to load HTML templates
templates = Jinja2Templates(directory="./app/templates")

openai.api_key = os.environ['OPENAI_API_KEY']
# Generate a random secret key
secret_key = secrets.token_hex(199)
# Set it as the Flask application's secret key
app.secret_key = secret_key

# Heroku provides the DATABASE_URL environment variable
DATABASE_URL = os.environ['DATABASE_URL']

while True:
    try:
        conn = psycopg2.connect(
            f"postgresql://{os.environ['user']}:{os.environ['password']}@"
            f"{os.environ['host']}:{os.environ['port']}/{os.environ['database']}"
        )
        cursor = conn.cursor()
        print(f'Database connection was successful ðŸ˜Ž\n')
        break
    except Exception as error:
        print(f'Connecting to database failed:\nError: {error} ðŸ˜­\n')
        time.sleep(3)

# Creating the SQL command to fetch all data from the memories table
memory_db = "SELECT * FROM memories"

# Executing the query and fetching all the data
cursor.execute(memory_db)

conversations_datas = cursor.fetchall()


# print(f'conversations_datas:\n{conversations_datas[9]}\n')


def find_conversation_by_id(id):
    for converse in conversations_datas:
        if converse[0] == id:  # Assuming 'id' is the first column in the OMR table
            print(f'conversation by id: {converse}')
            return converse


def find_index_converse(id):
    for i, conv in enumerate(conversations_datas):
        if isinstance(conv, dict) and conv.get('id') == id:
            return i
        elif isinstance(conv, tuple) and conv[0] == id:
            return i
    return None


app.include_router(conversation.router)
app.include_router(user.router)
app.include_router(auth.router)

# @app.get("/", status_code=status.HTTP_201_CREATED)
# async def root():
#     return {"Be Good Doing Good By Acting Good Â¡!Â¡": "Siisi Chacal ðŸ”¥ðŸ‘ŒðŸ¿ðŸ˜‡ðŸ’ªðŸ¿ðŸ”¥"}
#
#
# @app.get("/", status_code=status.HTTP_201_CREATED)
# async def root():
#     try:
#         # Assuming you want to redirect to the URL fetched from the environment variable
#         # redirect_url = os.environ['REDIRECT_URL_DBT_OPENAI']
#         redirect_url = os.environ['REDIRECT_URL_FASTAPI']
#         return RedirectResponse(url=redirect_url, status_code=status.HTTP_302_FOUND)
#     except KeyError:
#         raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#                             detail="Redirect URL not found in environment variables")


# Initialize an empty conversation chain
llm = ChatOpenAI(temperature=0.0, model="gpt-3.5-turbo-0301")  # Set your desired LLM model here
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory, verbose=False)
memory_summary = ConversationSummaryBufferMemory(llm=llm, max_token_limit=19)


# Function to generate LLM response
def generate_llm_response(user_message):
    # Assuming 'conversation' is initialized as a ChatOpenAI object
    return conversation.predict(input=user_message)


# FastAPI route
@app.route("/", methods=["GET", "POST"])
async def home(request: Request, form: TextAreaForm = Depends(), db: Session = Depends(get_db)):
    answer = None

    if request.method == "POST" and form.writing_text:
        user_input = form.writing_text

        # Use the LLM to generate a response based on user input
        response = generate_llm_response(user_input)

        answer = response['output'] if response else None

    return templates.TemplateResponse("home.html", {"request": request, "writing_text_form": form, "answer": answer,
                                                    "memory_load": memory.load_memory_variables({}),
                                                    "memory_buffer": memory.buffer_as_str,
                                                    "summary_buffer": memory_summary.load_memory_variables({}),
                                                    "date": datetime.now().strftime("%a %d %B %Y")})


@app.post('/app-start')
def answer(
        request: Request,
        prompt: str = Form(...),  # Use Form to declare the form parameter
        db: Session = Depends(get_db)
):
    memories = db.query(models.Memory).all()
    user_message = prompt

    # Create a list of JSON strings for each conversation
    conversation_strings = [memory.conversations_summary for memory in memories]

    # Combine the first 1 and last 15 entries into a valid JSON array
    qdocs = f"[{','.join(conversation_strings[:1] + conversation_strings[-9:])}]"

    # # Decode the JSON string
    # conversations_json = json.loads(qdocs) -> use this instead of 'qdocs' for 'memories' table

    # Convert 'created_at' values to string
    created_at_list = [str(memory.created_at) for memory in memories]

    # Include 'created_at' in the conversation context
    conversation_context = {
        "created_at": created_at_list[-9:],
        "conversations": qdocs,
        "user_message": user_message,
    }

    # Call llm ChatOpenAI
    response = conversation.predict(input=json.dumps(conversation_context))
    print(f'conversation_context:\n{conversation_context}\n')

    # Check if the response is a string, and if so, use it as the assistant's reply
    if isinstance(response, str):
        assistant_reply = response
    else:
        # If it's not a string, access the assistant's reply as you previously did
        assistant_reply = response.choices[0].message['content']

    # Convert the text response to speech using gTTS
    tts = gTTS(assistant_reply)

    # Create a temporary audio file
    audio_file_path = 'temp_audio.mp3'
    tts.save(audio_file_path)

    memory_summary.save_context({"input": f"{user_message}"}, {"output": f"{response}"})
    conversations_summary = memory_summary.load_memory_variables({})
    conversations_summary_str = json.dumps(conversations_summary)  # Convert to string

    current_time = datetime.now(pytz.timezone('Europe/Paris'))

    # Create a new Memory object with the data
    new_memory = Memory(
        user_message=user_message,
        llm_response=assistant_reply,
        conversations_summary=conversations_summary_str,
        created_at=current_time,
    )
    # Add the new memory to the session
    db.add(new_memory)
    # Commit changes to the database
    # db.commit()
    # db.refresh(new_memory)

    print(f'User Input: {user_message} ðŸ˜Ž')
    print(f'LLM Response:\n{assistant_reply} ðŸ˜\n')

    # Return the response as JSON, including both text and the path to the audio file

    return JSONResponse(
        content={
            "answer_text": assistant_reply,
            "answer_audio_path": audio_file_path,
        }
    )


@app.route('/audio')
def serve_audio():
    audio_file_path = 'temp_audio.mp3'
    return send_file(audio_file_path, as_attachment=True)


@app.get("/app-all", response_class=HTMLResponse, status_code=status.HTTP_201_CREATED)
async def get_all_conversations(request: Request, db: Session = Depends(get_db)):
    # cursor.execute("""SELECT * FROM memories""")
    # posts = cursor.fetchall()(db: Session = Depends(get_db)):

    # public:
    conversations = db.query(models.Memory).all()

    # Convert the list of Memory objects to a list of plain text representations
    conversations_text = [str(memory) for memory in conversations]

    print(f'all conversation:\n{conversations_text} ðŸ‘ŒðŸ¿\n')

    return (templates.TemplateResponse
            ("all-conversations.html",
             {
                 "request": request,
                 "data": conversations_text,
                 "date": datetime.now().strftime("%a %d %B %Y")
             }
             )
            )


@app.route('/app-history')
def show_story(request: Request, form: TextAreaForm = Depends(), db: Session = Depends(get_db)):
    summary_conversation = memory_summary.load_memory_variables({})
    memory_load = memory.load_memory_variables({})
    memory_buffer = memory.buffer_as_str

    print(f'memory_buffer_story:\n{memory_buffer}\n')
    print(f'memory_load_story:\n{memory_load}\n')
    print(f'summary_conversation_story:\n{summary_conversation}\n')

    return templates.TemplateResponse('show-history.html', {
        "request": request,
        "writing_text_form": form,
        "memory_load": memory_load,
        "memory_buffer": memory_buffer,
        "summary_conversation": summary_conversation,
        "date": datetime.now().strftime("%a %d %B %Y")
    })


if __name__ == '__main__':
    # Clean up any previous temporary audio files
    temp_audio_file = 'temp_audio.mp3'
    if os.path.exists(temp_audio_file):
        os.remove(temp_audio_file)

    uvicorn.run(app, host="0.0.0.0", port=int(os.environ.get("PORT", 8000)))
-->